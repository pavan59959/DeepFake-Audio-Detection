{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models, Model\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoFeatureExtractor, Wav2Vec2FeatureExtractor, TFWav2Vec2Model, Wav2Vec2Processor\n",
    "\n",
    "# Only log error messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "# Set random seed\n",
    "tf.keras.utils.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Basic configuration\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum duration of the input audio file we feed to our Wav2Vec 2.0 model.\n",
    "MAX_DURATION = 5  # Change to 30 seconds\n",
    "\n",
    "# Sampling rate is the number of samples of audio recorded every second\n",
    "SAMPLING_RATE = 16000\n",
    "BATCH_SIZE = 8  # Batch-size for training and evaluating our model.\n",
    "NUM_CLASSES = 2  # Number of classes our dataset will have (2 in our case).\n",
    "HIDDEN_DIM = 768  # Dimension of our model output (768 in case of Wav2Vec 2.0 - Base).\n",
    "MAX_SEQ_LENGTH = MAX_DURATION * SAMPLING_RATE  # Maximum length of the input audio file.\n",
    "\n",
    "# Wav2Vec 2.0 results in an output frequency with a stride of about 20ms.\n",
    "MAX_FRAMES = 249 # Adjust for 30 seconds\n",
    "MAX_EPOCHS = 10  # Maximum number of training epochs.\n",
    "SEED = 42\n",
    "MODEL_CHECKPOINT = \"facebook/pav2e-base\"  # Name of pretrained model from Hugging Face Model Hu\n",
    "\n",
    "FAKE = \"KAGGLE/AUDIO/FAKE\"\n",
    "REAL = \"KAGGLE/AUDIO/REAL\"\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/pav2e-base\")  #retrieve feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Load files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files_and_labels(fake_folder, real_folder):\n",
    "    audio_data = []\n",
    "    labels = []\n",
    "\n",
    "    # Load fake audio files\n",
    "    for filename in os.listdir(fake_folder):\n",
    "        file_path = os.path.join(fake_folder, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=16000, duration=MAX_DURATION)  # Load 30 seconds\n",
    "        audio_data.append(audio)\n",
    "        labels.append(1)\n",
    "\n",
    "    # Load real audio files\n",
    "    for filename in os.listdir(real_folder):\n",
    "        file_path = os.path.join(real_folder, filename)\n",
    "        audio, sr = librosa.load(file_path, sr=16000, duration=MAX_DURATION)  # Load 30 seconds\n",
    "        audio_data.append(audio)\n",
    "        labels.append(0)\n",
    "\n",
    "    return audio_data, np.array(labels)\n",
    "\n",
    "def extract_features(audio_data):\n",
    "    features = []\n",
    "    for audio in audio_data:\n",
    "        inputs = feature_extractor(audio, sampling_rate=16000, return_tensors=\"np\", padding=True, truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "        features.append(inputs.input_values)\n",
    "    return np.concatenate(features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 80000)\n"
     ]
    }
   ],
   "source": [
    "audio_data, labels = load_audio_files_and_labels(FAKE, REAL)\n",
    "features = extract_features(audio_data)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Create training and testing batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, y_tr, y_te = train_test_split(features, labels, test_size=0.2, stratify=labels, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Create custom Model using TFWav2Vec2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kai\\Documents\\Repo\\DeepFake-Audio-Detection\\myenv\\lib\\site-packages\\transformers\\configuration_utils.py:302: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "\n",
      "TFWav2Vec2Model has backpropagation operations that are NOT supported on CPU. If you wish to train/fine-tune this model, you need a GPU or a TPU\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFWav2Vec2Model: ['project_q.weight', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'quantizer.codevectors', 'project_hid.bias', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing TFWav2Vec2Model from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFWav2Vec2Model from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFWav2Vec2Model were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWav2Vec2Model for predictions without further training.\n",
      "c:\\Users\\Kai\\Documents\\Repo\\DeepFake-Audio-Detection\\myenv\\lib\\site-packages\\keras\\src\\layers\\layer.py:391: UserWarning: `build()` was called on layer 'wav2_vec2__model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kai\\Documents\\Repo\\DeepFake-Audio-Detection\\myenv\\lib\\site-packages\\keras\\src\\models\\functional.py:106: UserWarning: When providing `inputs` as a dict, all keys in the dict must match the names of the corresponding tensors. Received key 'input_values' mapping to value <KerasTensor shape=(None, 80000), dtype=float32, sparse=False, name=keras_tensor> which has name 'keras_tensor'. Change the tensor name to 'input_values' (via `Input(..., name='input_values')`)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kai\\Documents\\Repo\\DeepFake-Audio-Detection\\myenv\\lib\\site-packages\\keras\\src\\models\\functional.py:106: UserWarning: When providing `inputs` as a dict, all keys in the dict must match the names of the corresponding tensors. Received key 'attention_mask' mapping to value <KerasTensor shape=(None, 80000), dtype=int32, sparse=False, name=keras_tensor_1> which has name 'keras_tensor_1'. Change the tensor name to 'attention_mask' (via `Input(..., name='attention_mask')`)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mean_pool(hidden_states, feature_lengths, batch_size):\n",
    "    attenion_mask = tf.sequence_mask(\n",
    "        feature_lengths, maxlen=MAX_FRAMES, dtype=tf.dtypes.int64\n",
    "    )\n",
    "    padding_mask = tf.cast(\n",
    "        tf.reverse(tf.cumsum(tf.reverse(attenion_mask, [-1]), -1), [-1]),\n",
    "        dtype=tf.dtypes.bool,\n",
    "    )\n",
    "    hidden_states = tf.where(\n",
    "        tf.broadcast_to(\n",
    "            tf.expand_dims(~padding_mask, -1), (batch_size, MAX_FRAMES, HIDDEN_DIM)\n",
    "        ),\n",
    "        0.0,\n",
    "        hidden_states,\n",
    "    )\n",
    "    pooled_state = tf.math.reduce_sum(hidden_states, axis=1) / tf.reshape(\n",
    "        tf.math.reduce_sum(tf.cast(padding_mask, dtype=tf.dtypes.float32), axis=1),\n",
    "        [-1, 1],\n",
    "    )\n",
    "    return pooled_state\n",
    "\n",
    "class Wav2Vec2_Model(layers.Layer):\n",
    "\n",
    "    def __init__(self, model_checkpoint, num_classes):\n",
    "        super().__init__()\n",
    "        self.wav2vec2 = TFWav2Vec2Model.from_pretrained(\n",
    "            model_checkpoint, apply_spec_augment=False, from_pt=True\n",
    "        )\n",
    "        self.pooling = layers.GlobalAveragePooling1D()\n",
    "        self.intermediate_layer_dropout = layers.Dropout(0.5)\n",
    "        self.final_layer = layers.Dense(num_classes, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        hidden_states = self.wav2vec2(inputs[\"input_values\"])[0]\n",
    "        batch_size = tf.shape(hidden_states)[0]\n",
    "\n",
    "        if tf.is_tensor(inputs[\"attention_mask\"]):\n",
    "            audio_lengths = tf.cumsum(inputs[\"attention_mask\"], -1)[:, -1]\n",
    "            feature_lengths = self.wav2vec2.wav2vec2._get_feat_extract_output_lengths(\n",
    "                audio_lengths\n",
    "            )\n",
    "            pooled_state = mean_pool(hidden_states, feature_lengths, batch_size)\n",
    "        else:\n",
    "            pooled_state = self.pooling(hidden_states)\n",
    "\n",
    "        intermediate_state = self.intermediate_layer_dropout(pooled_state)\n",
    "        final_state = self.final_layer(intermediate_state)\n",
    "\n",
    "        return final_state\n",
    "\n",
    "# Rebuild the model to apply the mixed precision policy\n",
    "def build_model():\n",
    "    inputs = {\n",
    "        \"input_values\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"float32\"),\n",
    "        \"attention_mask\": tf.keras.Input(shape=(MAX_SEQ_LENGTH,), dtype=\"int32\"),\n",
    "    }\n",
    "    wav2vec2_model = Wav2Vec2_Model(MODEL_CHECKPOINT, NUM_CLASSES)(\n",
    "        inputs\n",
    "    )\n",
    "    model = tf.keras.Model(inputs, wav2vec2_model)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 2s/step - accuracy: 0.5688 - loss: 0.6786 - val_accuracy: 0.8462 - val_loss: 0.5331\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6345 - loss: 0.6361 - val_accuracy: 0.8462 - val_loss: 0.5299\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.7009 - loss: 0.6239 - val_accuracy: 0.8462 - val_loss: 0.5269\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.8719 - loss: 0.5341 - val_accuracy: 0.8462 - val_loss: 0.5239\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.6161 - loss: 0.6370 - val_accuracy: 0.8462 - val_loss: 0.5209\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6183 - loss: 0.6442 - val_accuracy: 0.8462 - val_loss: 0.5179\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 0.5381 - loss: 0.6440 - val_accuracy: 0.8462 - val_loss: 0.5149\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6280 - loss: 0.6144 - val_accuracy: 0.8462 - val_loss: 0.5120\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.8256 - loss: 0.5680 - val_accuracy: 0.8462 - val_loss: 0.5091\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 0.6902 - loss: 0.5705 - val_accuracy: 0.8462 - val_loss: 0.5063\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.data.Dataset.from_tensor_slices(({\"input_values\": X_tr, \"attention_mask\": np.ones_like(X_tr)}, y_tr)).batch(BATCH_SIZE)\n",
    "val_data = tf.data.Dataset.from_tensor_slices(({\"input_values\": X_te, \"attention_mask\": np.ones_like(X_te)}, y_te)).batch(BATCH_SIZE)\n",
    "\n",
    "# Training\n",
    "history = model.fit(\n",
    "    inputs,\n",
    "    validation_data=val_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Measure model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy List:  [0.6274510025978088, 0.7058823704719543, 0.7254902124404907, 0.8627451062202454, 0.686274528503418, 0.6274510025978088, 0.6470588445663452, 0.7058823704719543, 0.843137264251709, 0.7450980544090271]\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3s/step\n",
      "Accuracy: 0.8461538461538461\n",
      "Precision: 0.9090909090909091\n",
      "Recall: 0.9090909090909091\n",
      "F1-score: 0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Get training accuracy list\n",
    "training_accuracy = history.history['accuracy']\n",
    "print(\"Training Accuracy List: \", training_accuracy)\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "y_pred_probs = model.predict(\n",
    "    {\"input_values\": X_te, \"attention_mask\": np.ones_like(X_te)},\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_te, y_pred)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_te, y_pred, average='binary')\n",
    "recall = recall_score(y_te, y_pred, average='binary')\n",
    "f1 = f1_score(y_te, y_pred, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
